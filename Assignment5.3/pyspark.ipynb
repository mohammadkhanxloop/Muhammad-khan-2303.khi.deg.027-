{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions  as F\n",
    "from pyspark.sql.types import IntegerType, StringType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "scSpark = SparkSession.builder.appName(\"Assignment5.3\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-----+--------------------+------+---+-----+-----+----------------+-------+-----+--------+-------------------+\n",
      "|PassengerId|Survived|class|                Name|   Sex|Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|          TimeStamp|\n",
      "+-----------+--------+-----+--------------------+------+---+-----+-----+----------------+-------+-----+--------+-------------------+\n",
      "|          1|       0|    3|Braund, Mr. Owen ...|  male| 22|    1|    0|       A/5 21171|   7.25| null|       S|2020-01-01 13:45:25|\n",
      "|          2|       1|    1|Cumings, Mrs. Joh...|female| 38|    1|    0|        PC 17599|71.2833|  C85|       C|2020-01-01 13:44:48|\n",
      "|          3|       1|    3|Heikkinen, Miss. ...|female| 26|    0|    0|STON/O2. 3101282|  7.925| null|       S|2020-01-01 13:38:11|\n",
      "|          4|       1|    1|Futrelle, Mrs. Ja...|female| 35|    1|    0|          113803|   53.1| C123|       S|2020-01-01 13:32:00|\n",
      "|          5|       0|    3|Allen, Mr. Willia...|  male| 35|    0|    0|          373450|   8.05| null|       S|2020-01-01 13:36:30|\n",
      "+-----------+--------+-----+--------------------+------+---+-----+-----+----------------+-------+-----+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data from a source and create a DataFrame\n",
    "colunms = [\"PassengerId\",\"Survived\",\"class\",\"Name\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Ticket\",\"Fare\",\"Cabin\",\"Embarked\",\"TimeStamp\"]\n",
    "\n",
    "titanic_df = scSpark.read.csv('./data/titanic.csv', sep=',', inferSchema=True, header=False)\n",
    "titanic_df=titanic_df.toDF(*colunms)\n",
    "titanic_df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "For numerical columns, calculate minimum, maximum and average values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+-------------------+-----------------+\n",
      "|summary|      PassengerId|             class|               Age|             SibSp|              Parch|             Fare|\n",
      "+-------+-----------------+------------------+------------------+------------------+-------------------+-----------------+\n",
      "|  count|              891|               891|               714|               891|                891|              891|\n",
      "|   mean|            446.0| 2.308641975308642|29.679271708683473|0.5230078563411896|0.38159371492704824| 32.2042079685746|\n",
      "| stddev|257.3538420152301|0.8360712409770491|14.536482769437564|1.1027434322934315| 0.8060572211299488|49.69342859718089|\n",
      "|    min|                1|                 1|                 0|                 0|                  0|              0.0|\n",
      "|    max|              891|                 3|                80|                 8|                  6|         512.3292|\n",
      "+-------+-----------------+------------------+------------------+------------------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Colunms\n",
    "allcols = [cols[0] for cols in titanic_df]\n",
    "allcols\n",
    "\n",
    "# Separating colunms\n",
    "cat_cols = [item[0] for item in titanic_df.dtypes if item[1].startswith('string')]\n",
    "numeric_cols = [col for col in titanic_df.columns if col not in cat_cols]\n",
    "\n",
    "cat_df = titanic_df[cat_cols]\n",
    "numeric_df = titanic_df[numeric_cols[:-1]].drop('Survived')\n",
    "\n",
    "# Summary\n",
    "numeric_df.describe().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 & 3\n",
    "For categorical columns, create and apply UDF that will change the last letter of every word to “1”,\n",
    "\n",
    "And Sort DataFrame by the first column and save the results to the Parquet file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-----+--------------------+------+---+-----+-----+---------+-------+-----+--------+-------------------+\n",
      "|PassengerId|Survived|class|                Name|   Sex|Age|SibSp|Parch|   Ticket|   Fare|Cabin|Embarked|          TimeStamp|\n",
      "+-----------+--------+-----+--------------------+------+---+-----+-----+---------+-------+-----+--------+-------------------+\n",
      "|          1|       0|    1|Braund, Mr. Owen ...|  mal1| 22|    1|    1|A/5 21171|   7.25| null|       1|2020-01-01 13:45:25|\n",
      "|          2|       1|    1|Cumings, Mrs. Joh...|femal1| 38|    1|    1| PC 17599|71.2833|  C85|       1|2020-01-01 13:44:48|\n",
      "+-----------+--------+-----+--------------------+------+---+-----+-----+---------+-------+-----+--------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = ['Sex', 'Parch', 'class', 'Embarked']\n",
    "\n",
    "def word_change(words): \n",
    "    words = str(words)\n",
    "    if(len(words) == 1):\n",
    "        return words[0:-1] + '1'\n",
    "    return ' '.join([word[0:-1]+'1' for word in words.split()])\n",
    "\n",
    "word_change_udf = F.udf(word_change, StringType())\n",
    "for col in cols:\n",
    "    titanic_df =  titanic_df.withColumn(col, word_change_udf(titanic_df[col]))\n",
    "titanic_df.show(2)\n",
    "\n",
    "final_df =  titanic_df.sort(titanic_df[0], asc=False)\n",
    "final_df.write.mode(\"overwrite\").save('./data/titanic.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "office_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
